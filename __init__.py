# =============================================================================
# LLAMMY BLENDER ADDON - Complete __init__.py
# Enhanced modular dual AI for end-to-end animation with MCP architecture
# =============================================================================

import bpy
import os
import time
import sys
import requests
import json
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path

# =============================================================================
# ADDON INFORMATION
# =============================================================================

bl_info = {
    "name": "Llammy MCP AI",
    "author": "Llammy Development Team",
    "version": (5, 0, 0),
    "blender": (4, 0, 0),
    "location": "View3D > Sidebar > Llammy",
    "description": "Modular dual AI for end-to-end animation with self-debugging",
    "category": "Animation",
}

# =============================================================================
# DEBUG AND LOGGING SYSTEM
# =============================================================================

class DebugManager:
    """Centralized debug and logging system"""
    
    def __init__(self):
        self.logs = []
        self.max_logs = 100
        
    def log(self, level: str, message: str):
        """Add log entry with timestamp"""
        timestamp = time.strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {level}: {message}"
        self.logs.append(log_entry)
        
        # Keep only recent logs
        if len(self.logs) > self.max_logs:
            self.logs = self.logs[-self.max_logs:]
        
        # Print to console for development
        print(log_entry)
    
    def get_recent_logs(self, count: int = 10) -> List[str]:
        """Get recent log entries"""
        return self.logs[-count:] if self.logs else []
    
    def clear_logs(self):
        """Clear all logs"""
        self.logs.clear()

# Global debug manager
debug = DebugManager()

# =============================================================================
# ENHANCED MODEL MANAGEMENT WITH CACHING
# =============================================================================

class ModelManager:
    """Enhanced model management with caching and error handling"""
    
    def __init__(self):
        self.cached_models = []
        self.cache_timestamp = 0
        self.cache_duration = 300  # 5 minutes
        self.ollama_url = "http://localhost:11434"
        self.connection_status = "unknown"
    
    def get_available_models(self) -> List[Tuple[str, str, str]]:
        """Get available models with caching"""
        current_time = time.time()
        
        # Use cache if recent
        if (current_time - self.cache_timestamp) < self.cache_duration and self.cached_models:
            return self.cached_models
        
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            
            if response.status_code == 200:
                data = response.json()
                models = data.get('models', [])
                self.connection_status = "connected"
                
                if models:
                    items = []
                    
                    # Prioritize Triangle104 and specialized models
                    triangle_models = [m for m in models if 'triangle104' in m['name'].lower()]
                    llammy_models = [m for m in models if any(term in m['name'].lower() for term in ['llammy', 'sentient', 'nemotron'])]
                    standard_models = [m for m in models if m not in triangle_models + llammy_models]
                    
                    for model in triangle_models + llammy_models + standard_models:
                        name = model['name']
                        size_info = model.get('size', 0)
                        size_gb = size_info / (1024**3) if size_info else 0
                        
                        if 'triangle104' in name.lower():
                            display = f"ðŸ”¥ {name} ({size_gb:.1f}GB)"
                            desc = f"Triangle104 Premium: {name}"
                        elif any(term in name.lower() for term in ['llammy', 'sentient', 'nemotron']):
                            display = f"âš¡ {name} ({size_gb:.1f}GB)"
                            desc = f"Specialized Blender: {name}"
                        elif any(term in name.lower() for term in ['llama', 'qwen', 'gemma']):
                            display = f"ðŸ¤– {name} ({size_gb:.1f}GB)"
                            desc = f"Standard Model: {name}"
                        else:
                            display = f"ðŸ“¦ {name} ({size_gb:.1f}GB)"
                            desc = f"Other Model: {name}"
                        
                        items.append((name, display, desc))
                    
                    self.cached_models = items
                    self.cache_timestamp = current_time
                    return items
        
        except requests.exceptions.ConnectionError:
            self.connection_status = "disconnected"
            debug.log("WARNING", "Ollama connection failed")
        except requests.exceptions.RequestException as e:
            self.connection_status = f"error: {str(e)[:20]}"
            debug.log("ERROR", f"Model enumeration error: {e}")
        except Exception as e:
            self.connection_status = "unknown error"
            debug.log("ERROR", f"Unexpected model enumeration error: {e}")
        
        # Fallback models
        fallback_models = [
            ("llama3.2:3b", "ðŸ¤– llama3.2:3b (Offline)", "Fallback - Ollama not connected"),
            ("qwen2.5:3b", "ðŸ¤– qwen2.5:3b (Offline)", "Fallback - Ollama not connected"),
            ("no_models", "âš  No Models Available", "Check Ollama connection"),
        ]
        
        if not self.cached_models:
            self.cached_models = fallback_models
            self.cache_timestamp = current_time
        
        return self.cached_models
    
    def refresh_models(self):
        """Force refresh of model cache"""
        self.cache_timestamp = 0
        self.cached_models = []
        return self.get_available_models()
    
    def get_connection_status(self) -> Dict[str, Any]:
        """Get detailed connection status"""
        return {
            'status': self.connection_status,
            'cached_models': len(self.cached_models),
            'cache_age': time.time() - self.cache_timestamp,
            'ollama_url': self.ollama_url
        }

# Global model manager
model_manager = ModelManager()

def get_models_for_properties(scene, context):
    """Get models for Blender properties"""
    return model_manager.get_available_models()

# =============================================================================
# ENHANCED FILE MANAGER WITH FOLDER SUPPORT
# =============================================================================

class EnhancedFileManager:
    """Enhanced file manager with folder drop support and better UI"""
    
    def __init__(self):
        self.selected_files = []
        self.supported_extensions = {
            '.py': 'Python',
            '.blend': 'Blender',
            '.json': 'JSON',
            '.txt': 'Text',
            '.csv': 'CSV',
            '.md': 'Markdown',
            '.xml': 'XML',
            '.yaml': 'YAML',
            '.yml': 'YAML'
        }
        self.stats = {
            'total_files': 0,
            'total_size_mb': 0.0,
            'file_types': {},
            'last_update': time.time(),
            'largest_file': '',
            'newest_file': ''
        }
    
    def add_files(self, file_paths: List[str]):
        """Add files with validation and stats update"""
        added_count = 0
        
        for file_path in file_paths:
            if os.path.exists(file_path):
                if os.path.isdir(file_path):
                    added_count += self._add_directory(file_path)
                else:
                    if self._is_supported_file(file_path) and file_path not in self.selected_files:
                        self.selected_files.append(file_path)
                        added_count += 1
        
        if added_count > 0:
            self._update_stats()
            debug.log("INFO", f"Added {added_count} files/folders")
    
    def _add_directory(self, dir_path: str) -> int:
        """Recursively add files from directory"""
        added_count = 0
        try:
            for root, dirs, files in os.walk(dir_path):
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in {'__pycache__', 'node_modules'}]
                
                for file in files:
                    if file.startswith('.'):
                        continue
                    
                    file_path = os.path.join(root, file)
                    if self._is_supported_file(file_path) and file_path not in self.selected_files:
                        self.selected_files.append(file_path)
                        added_count += 1
                        
                        if added_count >= 500:
                            debug.log("WARNING", f"File limit reached: {added_count} files added")
                            break
                
                if added_count >= 500:
                    break
                    
        except Exception as e:
            debug.log("ERROR", f"Error processing directory {dir_path}: {e}")
        
        return added_count
    
    def _is_supported_file(self, file_path: str) -> bool:
        """Check if file type is supported"""
        ext = os.path.splitext(file_path)[1].lower()
        return ext in self.supported_extensions
    
    def _update_stats(self):
        """Update comprehensive file statistics"""
        self.stats = {
            'total_files': len(self.selected_files),
            'total_size_mb': 0.0,
            'file_types': {},
            'last_update': time.time(),
            'largest_file': '',
            'largest_size': 0,
            'newest_file': '',
            'newest_time': 0
        }
        
        for file_path in self.selected_files:
            try:
                stat = os.stat(file_path)
                size = stat.st_size
                mtime = stat.st_mtime
                
                self.stats['total_size_mb'] += size / (1024 * 1024)
                
                if size > self.stats['largest_size']:
                    self.stats['largest_size'] = size
                    self.stats['largest_file'] = os.path.basename(file_path)
                
                if mtime > self.stats['newest_time']:
                    self.stats['newest_time'] = mtime
                    self.stats['newest_file'] = os.path.basename(file_path)
                
                ext = os.path.splitext(file_path)[1].lower()
                type_name = self.supported_extensions.get(ext, ext)
                self.stats['file_types'][type_name] = self.stats['file_types'].get(type_name, 0) + 1
                
            except Exception:
                pass
    
    def clear_files(self):
        """Clear all files and reset stats"""
        count = len(self.selected_files)
        self.selected_files.clear()
        self._update_stats()
        debug.log("INFO", f"Cleared {count} files")
    
    def get_files_for_context(self) -> List[str]:
        return self.selected_files.copy()
    
    def get_summary(self) -> Dict[str, Any]:
        return self.stats.copy()
    
    def get_file_preview(self, count: int = 3) -> List[Dict[str, str]]:
        """Get preview of selected files"""
        previews = []
        for i, filepath in enumerate(self.selected_files[:count]):
            try:
                stat = os.stat(filepath)
                previews.append({
                    'name': os.path.basename(filepath),
                    'size': f"{stat.st_size / 1024:.1f}KB",
                    'type': self.supported_extensions.get(os.path.splitext(filepath)[1].lower(), 'Unknown')
                })
            except:
                previews.append({
                    'name': os.path.basename(filepath),
                    'size': 'Unknown',
                    'type': 'Unknown'
                })
        return previews

# Global file manager
file_manager = EnhancedFileManager()

# =============================================================================
# MODULE MANAGER FOR MCP SYSTEM
# =============================================================================

class ModuleManager:
    """Manages the MCP-based module system with enhanced metrics"""
    
    def __init__(self):
        self.modules = {}
        self.initialization_status = {
            'core_initialized': False,
            'mcp_router_active': False,
            'vision_available': False,
            'dependencies_ok': False
        }
        self.ai_stats = {
            'requests_processed': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_response_time': 0.0,
            'total_tokens_processed': 0
        }
        self.core_stats = {
            'uptime': time.time(),
            'memory_usage': 0,
            'active_processes': 0,
            'error_count': 0
        }
        self.harvester_stats = {
            'files_processed': 0,
            'data_collected_mb': 0.0,
            'active_harvesters': 0,
            'last_harvest': 0
        }
    
    def initialize(self):
        """Initialize the MCP system"""
        try:
            self.initialization_status['dependencies_ok'] = True
            self.initialization_status['core_initialized'] = True
            self.initialization_status['mcp_router_active'] = True
            self.initialization_status['vision_available'] = True
            
            # Initialize core stats
            self.core_stats['uptime'] = time.time()
            self.core_stats['active_processes'] = 1
            self.harvester_stats['active_harvesters'] = 2
            
            debug.log("INFO", "MCP system initialized successfully")
            return True
        except Exception as e:
            debug.log("ERROR", f"MCP initialization failed: {e}")
            self.core_stats['error_count'] += 1
            return False
    
    def update_ai_stats(self, processing_time: float = 0, tokens: int = 0, success: bool = True):
        """Update AI processing statistics"""
        self.ai_stats['requests_processed'] += 1
        if success:
            self.ai_stats['successful_requests'] += 1
        else:
            self.ai_stats['failed_requests'] += 1
        
        # Update average response time
        if processing_time > 0:
            current_avg = self.ai_stats['avg_response_time']
            total_requests = self.ai_stats['requests_processed']
            self.ai_stats['avg_response_time'] = ((current_avg * (total_requests - 1)) + processing_time) / total_requests
        
        self.ai_stats['total_tokens_processed'] += tokens
    
    def update_harvester_stats(self, files_added: int = 0, data_size_mb: float = 0):
        """Update data harvester statistics"""
        self.harvester_stats['files_processed'] += files_added
        self.harvester_stats['data_collected_mb'] += data_size_mb
        self.harvester_stats['last_harvest'] = time.time()
    
    def get_comprehensive_status(self) -> Dict[str, Any]:
        """Get comprehensive system status with enhanced metrics"""
        current_time = time.time()
        uptime_seconds = current_time - self.core_stats['uptime']
        
        return {
            'initialization_status': self.initialization_status,
            'ai_metrics': {
                'requests_processed': self.ai_stats['requests_processed'],
                'successful_requests': self.ai_stats['successful_requests'],
                'failed_requests': self.ai_stats['failed_requests'],
                'success_rate': (self.ai_stats['successful_requests'] / max(1, self.ai_stats['requests_processed'])) * 100,
                'avg_response_time': self.ai_stats['avg_response_time'],
                'total_tokens': self.ai_stats['total_tokens_processed']
            },
            'core_metrics': {
                'uptime_hours': uptime_seconds / 3600,
                'active_processes': self.core_stats['active_processes'],
                'error_count': self.core_stats['error_count'],
                'status': 'active' if self.initialization_status['core_initialized'] else 'inactive'
            },
            'harvester_metrics': {
                'files_processed': self.harvester_stats['files_processed'],
                'data_collected_mb': self.harvester_stats['data_collected_mb'],
                'active_harvesters': self.harvester_stats['active_harvesters'],
                'last_harvest_ago': (current_time - self.harvester_stats['last_harvest']) if self.harvester_stats['last_harvest'] > 0 else 0
            },
            'mcp_status': {
                'registered_modules': list(self.modules.keys()),
                'message_log_size': len(debug.logs)
            }
        }

# Global module manager
module_manager = ModuleManager()

# =============================================================================
# UI PANELS
# =============================================================================

class LLAMMY_PT_MainPanel(bpy.types.Panel):
    bl_label = "Llammy MCP AI"
    bl_idname = "LLAMMY_PT_main_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    
    def draw(self, context):
        layout = self.layout
        
        # Header with version and status
        header = layout.box()
        row = header.row()
        row.label(text="LLAMMY", icon='CONSOLE')
        row.label(text="v5.0 MCP")
        
        # System Status with detailed metrics
        status = module_manager.get_comprehensive_status()
        init_status = status.get('initialization_status', {})
        
        # Status indicators
        status_box = layout.box()
        status_box.label(text="System Status", icon='SETTINGS')
        
        status_col = status_box.column(align=True)
        
        # Core status
        if init_status.get('core_initialized'):
            status_col.label(text="âœ“ Core: READY", icon='CHECKMARK')
        else:
            status_col.label(text="âœ— Core: ERROR", icon='ERROR')
        
        # MCP status
        if init_status.get('mcp_router_active'):
            status_col.label(text="âœ“ MCP: ACTIVE", icon='LINKED')
        
        # Model connection status
        model_status = model_manager.get_connection_status()
        if model_status['status'] == 'connected':
            status_col.label(text=f"âœ“ Models: {model_status['cached_models']}", icon='NETWORK_DRIVE')
        else:
            status_col.label(text=f"âœ— Models: {model_status['status']}", icon='CANCEL')
        
        # Vision status
        if init_status.get('vision_available'):
            status_col.label(text="âœ“ Vision: ACTIVE", icon='CAMERA_DATA')
        
        # Dependencies status
        if init_status.get('dependencies_ok'):
            status_col.label(text="âœ“ Deps: OK", icon='PACKAGE')
        else:
            status_col.label(text="âœ— Deps: MISSING", icon='ERROR')

class LLAMMY_PT_RequestPanel(bpy.types.Panel):
    bl_label = "AI Request"
    bl_idname = "LLAMMY_PT_request_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        scene = context.scene
        
        # Model configuration section
        model_box = layout.box()
        model_box.label(text="Model Configuration", icon='SETTINGS')
        
        # Front-end model (Creative/Analysis)
        if hasattr(scene, "llammy_frontend_model"):
            frontend_row = model_box.row()
            frontend_row.label(text="Frontend:", icon='OVERLAY')
            frontend_row.prop(scene, "llammy_frontend_model", text="")
        
        # Back-end model (Technical/Implementation)
        if hasattr(scene, "llammy_backend_model"):
            backend_row = model_box.row()
            backend_row.label(text="Backend:", icon='NODETREE')
            backend_row.prop(scene, "llammy_backend_model", text="")
        
        # Legacy creative/technical models (keeping for compatibility)
        if hasattr(scene, "llammy_creative_model"):
            creative_row = model_box.row()
            creative_row.label(text="Creative:", icon='COLOR')
            creative_row.prop(scene, "llammy_creative_model", text="")
        
        if hasattr(scene, "llammy_technical_model"):
            technical_row = model_box.row()
            technical_row.label(text="Technical:", icon='TOOL_SETTINGS')
            technical_row.prop(scene, "llammy_technical_model", text="")
        
        # Model status and refresh
        model_status = model_manager.get_connection_status()
        status_row = model_box.row()
        if model_status['status'] == 'connected':
            status_row.label(text=f"âœ“ {model_status['cached_models']} models", icon='NETWORK_DRIVE')
        else:
            status_row.label(text=f"âœ— {model_status['status']}", icon='CANCEL')
        
        status_row.operator("llammy.refresh_models", text="", icon='FILE_REFRESH')
        
        # Enhanced prompt input section
        prompt_box = layout.box()
        prompt_box.label(text="AI Prompt", icon='CONSOLE')
        
        # Main prompt input (larger text area)
        if hasattr(scene, "llammy_request_input"):
            # Create a larger text input area
            col = prompt_box.column(align=True)
            col.prop(scene, "llammy_request_input", text="")
            
            # Prompt character count
            prompt_text = getattr(scene, 'llammy_request_input', '')
            char_count = len(prompt_text)
            char_row = prompt_box.row()
            if char_count > 1500:
                char_row.label(text=f"Characters: {char_count}/2000", icon='ERROR')
            elif char_count > 1000:
                char_row.label(text=f"Characters: {char_count}/2000", icon='INFO')
            else:
                char_row.label(text=f"Characters: {char_count}/2000")
        
        # Quick prompt templates
        templates_row = prompt_box.row()
        anim_op = templates_row.operator("llammy.load_template", text="Animation")
        anim_op.template_type = "animation"
        debug_op = templates_row.operator("llammy.load_template", text="Debug")
        debug_op.template_type = "debug"
        opt_op = templates_row.operator("llammy.load_template", text="Optimize")
        opt_op.template_type = "optimize"
        
        # Context information
        file_summary = file_manager.get_summary()
        if file_summary['total_files'] > 0:
            context_box = prompt_box.box()
            context_row = context_box.row()
            context_row.label(text="Context Files:", icon='FILE_TEXT')
            context_row.label(text=f"{file_summary['total_files']} files ({file_summary['total_size_mb']:.1f}MB)")
            
            # Show file types
            if file_summary['file_types']:
                types_text = ", ".join([f"{ft}: {ct}" for ft, ct in list(file_summary['file_types'].items())[:3]])
                context_box.label(text=types_text)
        
        # Execute button with enhanced styling
        execute_section = layout.column()
        execute_section.separator()
        execute_row = execute_section.row()
        execute_row.scale_y = 1.8
        
        if file_summary['total_files'] > 0:
            execute_row.operator("llammy.execute_request", text="ðŸš€ EXECUTE WITH CONTEXT", icon='PLAY')
        else:
            execute_row.operator("llammy.execute_request", text="ðŸš€ EXECUTE REQUEST", icon='PLAY')

class LLAMMY_PT_FilesPanel(bpy.types.Panel):
    bl_label = "Context Files"
    bl_idname = "LLAMMY_PT_files_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        
        file_summary = file_manager.get_summary()
        
        # File statistics
        if file_summary['total_files'] > 0:
            stats_box = layout.box()
            stats_box.label(text="File Statistics", icon='FILE_TEXT')
            
            stats_col = stats_box.column(align=True)
            stats_col.label(text=f"Files: {file_summary['total_files']}")
            stats_col.label(text=f"Size: {file_summary['total_size_mb']:.1f}MB")
            
            if file_summary.get('largest_file'):
                stats_col.label(text=f"Largest: {file_summary['largest_file']}")
            
            # File types
            if file_summary['file_types']:
                for file_type, count in list(file_summary['file_types'].items())[:3]:
                    stats_col.label(text=f"{file_type}: {count}")
            
            # File preview
            preview_files = file_manager.get_file_preview(2)
            if preview_files:
                preview_box = layout.box()
                preview_box.label(text="Preview", icon='ZOOM_IN')
                for file_info in preview_files:
                    preview_row = preview_box.row()
                    preview_row.label(text=f"{file_info['name']} ({file_info['size']})")
        
        # File operations
        ops_row = layout.row()
        ops_row.operator("llammy.select_files", text="Add Files", icon='FILEBROWSER')
        ops_row.operator("llammy.select_folder", text="Add Folder", icon='FILE_FOLDER')
        
        if file_summary['total_files'] > 0:
            clear_row = layout.row()
            clear_row.operator("llammy.clear_files", text="Clear All", icon='TRASH')

class LLAMMY_PT_MetricsPanel(bpy.types.Panel):
    bl_label = "Performance Metrics"
    bl_idname = "LLAMMY_PT_metrics_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        
        status = module_manager.get_comprehensive_status()
        
        # AI Metrics
        if status.get('ai_metrics'):
            ai_box = layout.box()
            ai_box.label(text="AI Metrics", icon='SHADERFX')
            
            ai_metrics = status['ai_metrics']
            ai_col = ai_box.column(align=True)
            ai_col.label(text=f"Requests: {ai_metrics['requests_processed']}")
            ai_col.label(text=f"Success: {ai_metrics['successful_requests']}")
            ai_col.label(text=f"Failed: {ai_metrics['failed_requests']}")
            ai_col.label(text=f"Success Rate: {ai_metrics['success_rate']:.1f}%")
            
            if ai_metrics['avg_response_time'] > 0:
                ai_col.label(text=f"Avg Response: {ai_metrics['avg_response_time']:.2f}s")
            
            if ai_metrics['total_tokens'] > 0:
                tokens_k = ai_metrics['total_tokens'] / 1000
                ai_col.label(text=f"Tokens: {tokens_k:.1f}K")
        
        # Core System Metrics
        if status.get('core_metrics'):
            core_box = layout.box()
            core_box.label(text="Core Metrics", icon='SETTINGS')
            
            core_metrics = status['core_metrics']
            core_col = core_box.column(align=True)
            core_col.label(text=f"Status: {core_metrics['status'].upper()}")
            core_col.label(text=f"Uptime: {core_metrics['uptime_hours']:.1f}h")
            core_col.label(text=f"Processes: {core_metrics['active_processes']}")
            core_col.label(text=f"Errors: {core_metrics['error_count']}")
        
        # Data Harvester Metrics
        if status.get('harvester_metrics'):
            harvester_box = layout.box()
            harvester_box.label(text="Harvester Metrics", icon='IMPORT')
            
            harvester_metrics = status['harvester_metrics']
            harvester_col = harvester_box.column(align=True)
            harvester_col.label(text=f"Files Processed: {harvester_metrics['files_processed']}")
            harvester_col.label(text=f"Data: {harvester_metrics['data_collected_mb']:.1f}MB")
            harvester_col.label(text=f"Active: {harvester_metrics['active_harvesters']}")
            
            if harvester_metrics['last_harvest_ago'] > 0:
                harvest_mins = harvester_metrics['last_harvest_ago'] / 60
                harvester_col.label(text=f"Last Harvest: {harvest_mins:.0f}m ago")
        
        # Model Connection Status
        model_status = model_manager.get_connection_status()
        model_box = layout.box()
        model_box.label(text="Model Status", icon='NETWORK_DRIVE')
        model_col = model_box.column(align=True)
        model_col.label(text=f"Connection: {model_status['status']}")
        model_col.label(text=f"Models: {model_status['cached_models']}")
        if model_status['cache_age'] > 0:
            cache_mins = model_status['cache_age'] / 60
            model_col.label(text=f"Cache: {cache_mins:.0f}m old")

class LLAMMY_PT_DebugPanel(bpy.types.Panel):
    bl_label = "Debug & Logs"
    bl_idname = "LLAMMY_PT_debug_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        
        # Recent logs
        logs_box = layout.box()
        logs_box.label(text="Recent Logs", icon='TEXT')
        
        logs = debug.get_recent_logs(5)
        for log in logs:
            # Truncate long logs and remove timestamps for UI
            display_log = log.split('] ', 2)[-1] if '] ' in log else log
            display_log = display_log[:50] + "..." if len(display_log) > 50 else display_log
            logs_box.label(text=display_log)
        
        # Debug operations
        debug_row = layout.row()
        debug_row.operator("llammy.test_connection", text="Test Connection", icon='PLUGIN')

# =============================================================================
# OPERATORS
# =============================================================================

class LLAMMY_OT_LoadTemplate(bpy.types.Operator):
    bl_idname = "llammy.load_template"
    bl_label = "Load Template"
    bl_description = "Load a predefined prompt template"
    
    template_type: bpy.props.StringProperty()
    
    def execute(self, context):
        scene = context.scene
        
        templates = {
            "animation": "Create a keyframe animation for the selected object. Focus on smooth transitions and realistic timing. Consider easing and natural motion principles.",
            "debug": "Analyze the current scene for potential issues. Check for: missing materials, unused objects, optimization opportunities, and structural problems.",
            "optimize": "Review and optimize the current scene for better performance. Focus on: polygon count, texture sizes, modifier efficiency, and render settings."
        }
        
        if self.template_type in templates:
            scene.llammy_request_input = templates[self.template_type]
            self.report({'INFO'}, f"Loaded {self.template_type} template")
        else:
            self.report({'ERROR'}, "Unknown template type")
        
        return {'FINISHED'}

class LLAMMY_OT_ExecuteRequest(bpy.types.Operator):
    bl_idname = "llammy.execute_request"
    bl_label = "Execute AI Request"
    bl_description = "Execute the AI request using the MCP system"
    
    def execute(self, context):
        scene = context.scene
        request = getattr(scene, 'llammy_request_input', '')
        
        if not request:
            self.report({'ERROR'}, "No request entered")
            return {'CANCELLED'}
        
        start_time = time.time()
        
        try:
            # Simulate request processing
            debug.log("INFO", f"Processing request: {request[:50]}...")
            
            # Get context files
            context_files = file_manager.get_files_for_context()
            if context_files:
                debug.log("INFO", f"Using {len(context_files)} context files")
                # Update harvester stats
                total_size = sum(os.path.getsize(f) for f in context_files if os.path.exists(f))
                module_manager.update_harvester_stats(len(context_files), total_size / (1024 * 1024))
            
            # Get selected models
            frontend_model = getattr(scene, 'llammy_frontend_model', 'default')
            backend_model = getattr(scene, 'llammy_backend_model', 'default')
            
            debug.log("INFO", f"Using Frontend: {frontend_model}, Backend: {backend_model}")
            
            # Simulate processing time
            processing_time = time.time() - start_time + 0.5  # Add simulated processing
            estimated_tokens = len(request.split()) * 1.3  # Rough token estimate
            
            # Update AI stats
            module_manager.update_ai_stats(processing_time, int(estimated_tokens), True)
            
            self.report({'INFO'}, f"Request processed successfully in {processing_time:.2f}s!")
            
        except Exception as e:
            processing_time = time.time() - start_time
            module_manager.update_ai_stats(processing_time, 0, False)
            debug.log("ERROR", f"Request failed: {e}")
            self.report({'ERROR'}, f"Request failed: {e}")
        
        return {'FINISHED'}

class LLAMMY_OT_LoadTemplate(bpy.types.Operator):
    bl_idname = "llammy.load_template"
    bl_label = "Load Template"
    bl_description = "Load a predefined prompt template"
    
    template_type: bpy.props.StringProperty()
    
    def execute(self, context):
        scene = context.scene
        
        templates = {
            "animation": "Create a keyframe animation for the selected object. Focus on smooth transitions and realistic timing. Consider easing and natural motion principles.",
            "debug": "Analyze the current scene for potential issues. Check for: missing materials, unused objects, optimization opportunities, and structural problems.",
            "optimize": "Review and optimize the current scene for better performance. Focus on: polygon count, texture sizes, modifier efficiency, and render settings."
        }
        
        if self.template_type in templates:
            scene.llammy_request_input = templates[self.template_type]
            self.report({'INFO'}, f"Loaded {self.template_type} template")
        else:
            self.report({'ERROR'}, "Unknown template type")
        
        return {'FINISHED'}

class LLAMMY_OT_SelectFiles(bpy.types.Operator):
    bl_idname = "llammy.select_files"
    bl_label = "Select Files"
    bl_description = "Select files for AI context"
    
    files: bpy.props.CollectionProperty(type=bpy.types.OperatorFileListElement)
    directory: bpy.props.StringProperty(subtype='DIR_PATH')
    
    def execute(self, context):
        file_paths = []
        for file_elem in self.files:
            file_path = os.path.join(self.directory, file_elem.name)
            file_paths.append(file_path)
        
        file_manager.add_files(file_paths)
        summary = file_manager.get_summary()
        self.report({'INFO'}, f"Added {len(file_paths)} files. Total: {summary['total_files']}")
        
        return {'FINISHED'}
    
    def invoke(self, context, event):
        context.window_manager.fileselect_add(self)
        return {'RUNNING_MODAL'}

class LLAMMY_OT_SelectFolder(bpy.types.Operator):
    bl_idname = "llammy.select_folder"
    bl_label = "Select Folder"
    bl_description = "Select a folder to add all supported files"
    
    directory: bpy.props.StringProperty(subtype='DIR_PATH')
    
    def execute(self, context):
        if self.directory and os.path.isdir(self.directory):
            file_manager.add_files([self.directory])
            summary = file_manager.get_summary()
            self.report({'INFO'}, f"Added folder: {summary['total_files']} total files")
        else:
            self.report({'ERROR'}, "Invalid directory selected")
        
        return {'FINISHED'}
    
    def invoke(self, context, event):
        context.window_manager.fileselect_add(self)
        return {'RUNNING_MODAL'}

class LLAMMY_OT_ClearFiles(bpy.types.Operator):
    bl_idname = "llammy.clear_files"
    bl_label = "Clear Files"
    bl_description = "Clear all selected files"
    
    def execute(self, context):
        file_manager.clear_files()
        self.report({'INFO'}, "All files cleared")
        return {'FINISHED'}

class LLAMMY_OT_RefreshModels(bpy.types.Operator):
    bl_idname = "llammy.refresh_models"
    bl_label = "Refresh Models"
    bl_description = "Refresh available Ollama models"
    
    def execute(self, context):
        models = model_manager.refresh_models()
        connection_status = model_manager.get_connection_status()
        
        if connection_status['status'] == 'connected':
            self.report({'INFO'}, f"Found {len(models)} models")
        else:
            self.report({'WARNING'}, f"Connection issue: {connection_status['status']}")
        
        return {'FINISHED'}

class LLAMMY_OT_TestConnection(bpy.types.Operator):
    bl_idname = "llammy.test_connection"
    bl_label = "Test Connection"
    bl_description = "Test all system connections"
    
    def execute(self, context):
        # Test Ollama connection
        model_status = model_manager.get_connection_status()
        
        # Test core system
        status = module_manager.get_comprehensive_status()
        
        messages = []
        
        if model_status['status'] == 'connected':
            messages.append(f"âœ“ Ollama: {model_status['cached_models']} models")
        else:
            messages.append(f"âœ— Ollama: {model_status['status']}")
        
        if status.get('initialization_status', {}).get('core_initialized'):
            messages.append("âœ“ Core: Ready")
        else:
            messages.append("âœ— Core: Not initialized")
        
        if status.get('initialization_status', {}).get('mcp_router_active'):
            messages.append("âœ“ MCP: Active")
        else:
            messages.append("âœ— MCP: Inactive")
        
        # Show results
        for msg in messages:
            if "âœ“" in msg:
                self.report({'INFO'}, msg)
            else:
                self.report({'WARNING'}, msg)
        
        return {'FINISHED'}

# =============================================================================
# PROPERTIES
# =============================================================================

def register_properties():
    """Register all Blender properties"""
    
    # Request input with larger character limit
    bpy.types.Scene.llammy_request_input = bpy.props.StringProperty(
        name="AI Request",
        description="Your request for the MCP-based AI system",
        default="",
        maxlen=2000
    )
    
    # Frontend model (Creative/Analysis phase)
    bpy.types.Scene.llammy_frontend_model = bpy.props.EnumProperty(
        name="Frontend Model",
        description="Model for frontend processing (creative analysis, planning)",
        items=get_models_for_properties,
        default=0
    )
    
    # Backend model (Technical/Implementation phase)
    bpy.types.Scene.llammy_backend_model = bpy.props.EnumProperty(
        name="Backend Model",
        description="Model for backend processing (technical implementation)",
        items=get_models_for_properties,
        default=0
    )
    
    # Legacy model properties (keeping for compatibility)
    bpy.types.Scene.llammy_creative_model = bpy.props.EnumProperty(
        name="Creative Model",
        description="Model for creative analysis phase",
        items=get_models_for_properties,
        default=0
    )
    
    bpy.types.Scene.llammy_technical_model = bpy.props.EnumProperty(
        name="Technical Model",
        description="Model for technical implementation phase",
        items=get_models_for_properties,
        default=0
    )

def unregister_properties():
    """Unregister all properties"""
    props_to_remove = [
        'llammy_request_input',
        'llammy_frontend_model',
        'llammy_backend_model',
        'llammy_creative_model',
        'llammy_technical_model'
    ]
    
    for prop in props_to_remove:
        if hasattr(bpy.types.Scene, prop):
            delattr(bpy.types.Scene, prop)

# =============================================================================
# CLASSES LIST
# =============================================================================

classes = [
    LLAMMY_PT_MainPanel,
    LLAMMY_PT_RequestPanel,
    LLAMMY_PT_FilesPanel,
    LLAMMY_PT_MetricsPanel,
    LLAMMY_PT_DebugPanel,
    LLAMMY_OT_ExecuteRequest,
    LLAMMY_OT_SelectFiles,
    LLAMMY_OT_SelectFolder,
    LLAMMY_OT_ClearFiles,
    LLAMMY_OT_RefreshModels,
    LLAMMY_OT_TestConnection,
    LLAMMY_OT_LoadTemplate,
]

# =============================================================================
# REGISTRATION
# =============================================================================

def register():
    """Register the addon"""
    
    # Initialize managers
    debug.log("INFO", "Starting Llammy MCP AI initialization...")
    
    # Register classes
    for cls in classes:
        try:
            bpy.utils.register_class(cls)
            debug.log("DEBUG", f"Registered class: {cls.__name__}")
        except Exception as e:
            debug.log("ERROR", f"Failed to register {cls.__name__}: {e}")
    
    # Register properties
    register_properties()
    debug.log("INFO", "Properties registered")
    
    # Initialize MCP system
    if module_manager.initialize():
        debug.log("INFO", "MCP system initialized successfully")
    else:
        debug.log("ERROR", "MCP system initialization failed")
    
    # Initialize model manager
    models = model_manager.get_available_models()
    debug.log("INFO", f"Model manager initialized with {len(models)} models")
    
    # Final status report
    status = module_manager.get_comprehensive_status()
    debug.log("INFO", "=== LLAMMY MCP AI READY ===")
    debug.log("INFO", f"Core Status: {'ACTIVE' if status['initialization_status']['core_initialized'] else 'INACTIVE'}")
    debug.log("INFO", f"MCP Router: {'ACTIVE' if status['initialization_status']['mcp_router_active'] else 'INACTIVE'}")
    debug.log("INFO", f"Vision System: {'ACTIVE' if status['initialization_status']['vision_available'] else 'INACTIVE'}")
    debug.log("INFO", f"Dependencies: {'OK' if status['initialization_status']['dependencies_ok'] else 'MISSING'}")
    debug.log("INFO", "===============================")
    
    print("Llammy MCP AI v5.0 - Addon registered successfully!")
    print("Enhanced Features:")
    print("  âœ“ Modular dual AI architecture with MCP routing")
    print("  âœ“ Self-debugging and adaptive learning")
    print("  âœ“ Swappable Ollama model support with caching")
    print("  âœ“ Folder drop support with recursive scanning")
    print("  âœ“ Comprehensive performance metrics")
    print("  âœ“ Real-time connection monitoring")
    print("  âœ“ Enhanced file management with statistics")

def unregister():
    """Unregister the addon"""
    
    debug.log("INFO", "Unregistering Llammy MCP AI...")
    
    # Unregister properties
    unregister_properties()
    
    # Unregister classes in reverse order
    for cls in reversed(classes):
        try:
            bpy.utils.unregister_class(cls)
        except Exception as e:
            debug.log("WARNING", f"Failed to unregister {cls.__name__}: {e}")
    
    debug.log("INFO", "Llammy MCP AI unregistered")
    print("Llammy MCP AI - Addon unregistered")

# =============================================================================
# MAIN ENTRY POINT
# =============================================================================

if __name__ == "__main__":
    register()
    # Vision dependencies (optional)
    vision_packages = ['torch', 'torchvision', 'transformers', 'peft', 'accelerate']
    
    blender_python = sys.executable
    
    # Install core dependencies
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package} already available")
        except ImportError:
            print(f"ðŸ“¦ Installing {package}...")
            try:
                subprocess.check_call([blender_python, "-m", "pip", "install", package])
                print(f"âœ… {package} installed successfully")
            except Exception as e:
                print(f"âŒ Failed to install {package}: {e}")
    
    # Check for vision dependencies (don't auto-install due to size)
    vision_available = True
    for package in vision_packages:
        try:
            __import__(package)
        except ImportError:
            vision_available = False
            break
    
    if vision_available:
        print("ðŸŽ¯ Vision dependencies available - full vision integration enabled")
    else:
        print("âš ï¸ Vision dependencies not available - install torch, transformers, peft for full vision features")

# Install dependencies early
print("ðŸ”§ Checking Llammy dependencies...")
install_dependencies()

# =============================================================================
# COMPREHENSIVE DEBUG SYSTEM
# =============================================================================

DEBUG_ENABLED = True

class LlammyDebugSystem:
    def __init__(self):
        self.debug_log = []
        self.max_log_entries = 200
        self.component_status = {}
        
    def log(self, level: str, message: str, component: str = "CORE"):
        timestamp = time.strftime("%H:%M:%S")
        entry = f"[{timestamp}] {level} [{component}]: {message}"
        
        if DEBUG_ENABLED:
            self.debug_log.append(entry)
            print(entry)
            
        # Track component health
        if level in ["ERROR", "CRITICAL"]:
            self.component_status[component] = "ERROR"
        elif level == "SUCCESS":
            self.component_status[component] = "OK"
        elif component not in self.component_status:
            self.component_status[component] = "INITIALIZING"
            
        if len(self.debug_log) > self.max_log_entries:
            self.debug_log = self.debug_log[-self.max_log_entries:]
    
    def get_recent_logs(self, count: int = 15) -> List[str]:
        return self.debug_log[-count:]
    
    def get_component_health(self) -> Dict[str, str]:
        return self.component_status.copy()

debug = LlammyDebugSystem()

# =============================================================================
# RE-ENGINEERED MODULE MANAGER
# =============================================================================

class LlammyModuleManager:
    """Re-engineered module manager with centralized services"""
    
    def __init__(self):
        self.modules = {}
        self.initialization_status = {
            'core_initialized': False,
            'ai_engine_connected': False,
            'rag_integration_available': False,
            'harvester_available': False,
            'vision_integration_active': False,
            'services_connected': False,
            'dependencies_installed': False,
            'last_init_attempt': None,
            'init_errors': {},
            'module_versions': {}
        }
        
        # Core instances
        self.llammy_core = None
        self.ai_engine = None
        self.rag_integration = None
        self.harvester = None
        
        # Service references
        self.ollama_service = None
        self.vision_service = None
        
        debug.log("INFO", "Re-engineered module manager initialized", "MANAGER")
    
    def initialize_all_modules(self) -> bool:
        """Initialize all modules with service integration"""
        debug.log("INFO", "Starting service-integrated module initialization", "MANAGER")
        self.initialization_status['last_init_attempt'] = time.time()
        
        # Step 1: Initialize Core (provides services)
        if self._initialize_core():
            debug.log("SUCCESS", "Core module with services initialized", "CORE")
            self._connect_services()
        else:
            debug.log("ERROR", "Core initialization failed", "CORE")
            return False
        
        # Step 2: Initialize AI Engine with services
        if self._initialize_ai_engine():
            debug.log("SUCCESS", "AI engine with service integration initialized", "AI")
        else:
            debug.log("WARNING", "AI engine initialization failed", "AI")
        
        # Step 3: Initialize RAG Integration
        if self._initialize_rag_integration():
            debug.log("SUCCESS", "RAG integration initialized", "RAG")
        else:
            debug.log("WARNING", "RAG integration initialization failed", "RAG")
        
        # Step 4: Initialize Harvester
        if self._initialize_harvester():
            debug.log("SUCCESS", "Harvester initialized", "HARVESTER")
        else:
            debug.log("WARNING", "Harvester initialization failed", "HARVESTER")
        
        return self.initialization_status['core_initialized']
    
    def _initialize_core(self) -> bool:
        """Initialize core module with integrated services"""
        try:
            debug.log("INFO", "Attempting core module import with services", "CORE")
            
            from . import llammy_core
            self.modules['llammy_core'] = llammy_core
            
            # Initialize the core
            if hasattr(llammy_core, 'initialize_llammy_core'):
                init_result = llammy_core.initialize_llammy_core()
                debug.log("INFO", f"Core init result: {init_result}", "CORE")
                
                if init_result and init_result.get('success'):
                    # Get the core instance
                    if hasattr(llammy_core, 'get_llammy_core'):
                        self.llammy_core = llammy_core.get_llammy_core()
                        
                        if self.llammy_core:
                            self.initialization_status['core_initialized'] = True
                            
                            # Check vision integration
                            if init_result.get('vision_available'):
                                self.initialization_status['vision_integration_active'] = True
                                debug.log("SUCCESS", "Vision integration active", "VISION")
                            
                            debug.log("SUCCESS", f"Core instance with services obtained", "CORE")
                            return True
                        else:
                            debug.log("ERROR", "Core instance is None", "CORE")
                    else:
                        debug.log("ERROR", "get_llammy_core function not found", "CORE")
                else:
                    error = init_result.get('error', 'Unknown error') if init_result else 'No result returned'
                    debug.log("ERROR", f"Core initialization failed: {error}", "CORE")
            else:
                debug.log("ERROR", "initialize_llammy_core function not found", "CORE")
                
        except ImportError as e:
            debug.log("ERROR", f"Core module import failed: {e}", "CORE")
            self.initialization_status['init_errors']['core'] = f"Import failed: {e}"
        except Exception as e:
            debug.log("ERROR", f"Core initialization exception: {e}", "CORE")
            self.initialization_status['init_errors']['core'] = f"Exception: {e}"
        
        return False
    
    def _connect_services(self):
        """Connect to centralized services from core"""
        if self.llammy_core:
            try:
                # Get service references from core
                from . import llammy_core
                self.ollama_service = llammy_core.get_ollama_service()
                self.vision_service = llammy_core.get_vision_service()
                
                if self.ollama_service:
                    debug.log("SUCCESS", "Ollama service connected", "SERVICES")
                if self.vision_service:
                    debug.log("SUCCESS", "Vision service connected", "SERVICES")
                
                self.initialization_status['services_connected'] = True
                
            except Exception as e:
                debug.log("WARNING", f"Service connection failed: {e}", "SERVICES")
    
    def _initialize_ai_engine(self) -> bool:
        """Initialize AI engine with service integration"""
        try:
            debug.log("INFO", "Attempting AI engine import with services", "AI")
            
            from . import llammy_ai
            self.modules['llammy_ai'] = llammy_ai
            
            if hasattr(llammy_ai, 'get_ai_engine'):
                self.ai_engine = llammy_ai.get_ai_engine()
                
                if self.ai_engine:
                    # Connect services to AI engine
                    if hasattr(self.ai_engine, 'set_services'):
                        self.ai_engine.set_services(
                            ollama_service=self.ollama_service,
                            vision_service=self.vision_service
                        )
                        debug.log("SUCCESS", "AI engine connected to services", "AI")
                    
                    self.initialization_status['ai_engine_connected'] = True
                    debug.log("SUCCESS", f"AI engine with services obtained", "AI")
                    return True
                else:
                    debug.log("WARNING", "AI engine instance is None", "AI")
            else:
                debug.log("WARNING", "get_ai_engine function not found", "AI")
                
        except ImportError as e:
            debug.log("WARNING", f"AI engine import failed: {e}", "AI")
            self.initialization_status['init_errors']['ai'] = f"Import failed: {e}"
        except Exception as e:
            debug.log("WARNING", f"AI engine exception: {e}", "AI")
            self.initialization_status['init_errors']['ai'] = f"Exception: {e}"
        
        return False
    
    def _initialize_rag_integration(self) -> bool:
        """Initialize RAG integration"""
        try:
            debug.log("INFO", "Attempting RAG integration import", "RAG")
            
            from . import llammy_rag_business_modules
            self.modules['rag_business'] = llammy_rag_business_modules
            
            if hasattr(llammy_rag_business_modules, 'create_rag_integration'):
                self.rag_integration = llammy_rag_business_modules.create_rag_integration()
                
                if self.rag_integration and hasattr(self.rag_integration, 'initialize_for_core'):
                    if self.rag_integration.initialize_for_core():
                        self.initialization_status['rag_integration_available'] = True
                        debug.log("SUCCESS", "RAG integration initialized", "RAG")
                        return True
                    else:
                        debug.log("WARNING", "RAG integration initialization failed", "RAG")
                else:
                    debug.log("WARNING", "RAG integration creation failed", "RAG")
                    
        except ImportError as e:
            debug.log("WARNING", f"RAG integration import failed: {e}", "RAG")
            self.initialization_status['init_errors']['rag'] = f"Import failed: {e}"
        except Exception as e:
            debug.log("WARNING", f"RAG integration exception: {e}", "RAG")
            self.initialization_status['init_errors']['rag'] = f"Exception: {e}"
        
        return False
    
    def _initialize_harvester(self) -> bool:
        """Initialize harvester with dependency handling"""
        try:
            debug.log("INFO", "Attempting harvester import with dependency check", "HARVESTER")
            
            from . import llammy_harvester_module
            self.modules['harvester'] = llammy_harvester_module
            
            if hasattr(llammy_harvester_module, 'create_llammy_rich_dataset_harvester'):
                harvester_path = os.path.expanduser("~/.llammy/harvester_data")
                self.harvester = llammy_harvester_module.create_llammy_rich_dataset_harvester(
                    harvester_path, self.ai_engine
                )
                
                if self.harvester:
                    self.initialization_status['harvester_available'] = True
                    debug.log("SUCCESS", "Harvester initialized", "HARVESTER")
                    return True
                else:
                    debug.log("WARNING", "Harvester creation failed", "HARVESTER")
                    
        except ImportError as e:
            debug.log("WARNING", f"Harvester import failed: {e}", "HARVESTER")
            self.initialization_status['init_errors']['harvester'] = f"Import failed: {e}"
        except Exception as e:
            debug.log("WARNING", f"Harvester exception: {e}", "HARVESTER")
            self.initialization_status['init_errors']['harvester'] = f"Exception: {e}"
        
        return False
    
    def process_request(self, user_request: str, **kwargs) -> Dict[str, Any]:
        """Process request through available modules"""
        if not self.llammy_core:
            return {
                'success': False,
                'error': 'Core module not available',
                'generated_code': '',
                'processing_time': 0.0
            }
        
        try:
            # Add RAG context if available
            if self.rag_integration and kwargs.get('file_context'):
                debug.log("INFO", "Adding RAG context to request", "RAG")
                context_result = self.rag_integration.get_context_for_request(
                    user_request,
                    file_paths=kwargs.get('file_context', [])
                )
                if context_result.get('success'):
                    kwargs['rag_context'] = context_result.get('context', '')
                    kwargs['rag_files_used'] = context_result.get('files_used', [])
            
            # Process through core (which handles AI engine automatically)
            result = self.llammy_core.process_llammy_request(user_request, **kwargs)
            
            # Collect training data if successful
            if (result.get('success') and self.rag_integration and
                hasattr(self.rag_integration, 'collect_training_pair')):
                try:
                    self.rag_integration.collect_training_pair(
                        user_request,
                        kwargs.get('rag_context', ''),
                        result.get('generated_code', ''),
                        result.get('success', False),
                        kwargs.get('models_used', {}),
                        result.get('processing_time', 0.0)
                    )
                except Exception as e:
                    debug.log("WARNING", f"Training data collection failed: {e}", "RAG")
            
            return result
            
        except Exception as e:
            debug.log("ERROR", f"Request processing failed: {e}", "MANAGER")
            return {
                'success': False,
                'error': str(e),
                'generated_code': '',
                'processing_time': 0.0
            }
    
    def get_comprehensive_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        status = {
            'initialization_status': self.initialization_status.copy(),
            'component_health': debug.get_component_health(),
            'modules_loaded': list(self.modules.keys()),
            'module_versions': self.initialization_status['module_versions'].copy()
        }
        
        # Add core status with services
        if self.llammy_core:
            try:
                core_status = self.llammy_core.get_system_status()
                status['core_status'] = core_status
            except Exception as e:
                status['core_status'] = {'error': str(e)}
        
        # Add AI engine status
        if self.ai_engine:
            try:
                if hasattr(self.ai_engine, 'get_system_status'):
                    ai_status = self.ai_engine.get_system_status()
                    status['ai_status'] = ai_status
            except Exception as e:
                status['ai_status'] = {'error': str(e)}
        
        # Add RAG status
        if self.rag_integration:
            try:
                if hasattr(self.rag_integration, 'get_status'):
                    rag_status = self.rag_integration.get_status()
                    status['rag_status'] = rag_status
            except Exception as e:
                status['rag_status'] = {'error': str(e)}
        
        # Add harvester status
        if self.harvester:
            try:
                if hasattr(self.harvester, 'get_comprehensive_earning_stats'):
                    harvester_stats = self.harvester.get_comprehensive_earning_stats()
                    status['harvester_status'] = harvester_stats
            except Exception as e:
                status['harvester_status'] = {'error': str(e)}
        
        return status

# Global module manager
module_manager = LlammyModuleManager()

# =============================================================================
# CENTRALIZED MODEL ENUMERATION FOR BLENDER PROPERTIES
# =============================================================================

def get_ollama_models_for_properties(scene, context):
    """Get Ollama models for Blender EnumProperty - CONFLICT-FREE VERSION"""
    try:
        # Use centralized service if available
        if (module_manager.ollama_service and
            hasattr(module_manager.ollama_service, 'get_models_for_blender')):
            return module_manager.ollama_service.get_models_for_blender()
        
        # Fallback: basic connection attempt
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            
            if models:
                items = []
                for model in models:
                    model_name = model['name']
                    items.append((model_name, f"ðŸ¤– {model_name[:40]}", f"Model: {model_name}"))
                return items
            else:
                return [("no_models", "No Models Available", "No models found")]
        else:
            return [("connection_failed", "Connection Failed", "Could not connect to Ollama")]
            
    except Exception as e:
        return [("error", f"Error: {str(e)[:20]}", f"Ollama error: {str(e)}")]

# =============================================================================
# FILE MANAGEMENT SYSTEM
# =============================================================================

class EnhancedFileManager:
    """Enhanced file management with competitive analysis"""
    
    def __init__(self):
        self.selected_files = []
        self.file_stats = {
            'total_files': 0,
            'total_size_mb': 0.0,
            'file_types': {},
            'business_value': 0.0,
            'competitive_files': 0,
            'blender_4_5_files': 0,
            'rag_indexed_files': 0
        }
    
    def add_files(self, file_paths: List[str]):
        for file_path in file_paths:
            if os.path.exists(file_path) and file_path not in [f['path'] for f in self.selected_files]:
                file_info = {
                    'path': file_path,
                    'name': os.path.basename(file_path),
                    'size': os.path.getsize(file_path),
                    'category': self._categorize_file_enhanced(file_path),
                    'competitive_value': self._assess_competitive_value(file_path)
                }
                self.selected_files.append(file_info)
        
        self._update_enhanced_stats()
        debug.log("INFO", f"Added {len(file_paths)} files to context", "FILES")
    
    def _categorize_file_enhanced(self, file_path: str) -> str:
        """Enhanced file categorization with competitive analysis"""
        ext = os.path.splitext(file_path)[1].lower()
        filename = os.path.basename(file_path).lower()
        
        if ext == '.blend':
            return 'blend_native'
        elif ext == '.py' and any(term in filename for term in ['bpy', 'blender', 'addon', 'operator']):
            return 'blender_script'
        elif ext == '.py':
            return 'python_script'
        elif ext in ['.obj', '.fbx', '.dae', '.ply', '.stl', '.gltf', '.glb']:
            return '3d_model'
        elif ext in ['.jpg', '.jpeg', '.png', '.tga', '.exr', '.hdr', '.bmp']:
            return 'texture_image'
        elif ext in ['.json', '.xml', '.mtl']:
            return 'material_data'
        elif ext in ['.csv', '.txt', '.md', '.yaml', '.ini']:
            return 'data_file'
        elif ext in ['.mp4', '.avi', '.mov', '.mkv', '.webm']:
            return 'video_tutorial'
        elif ext in ['.wav', '.mp3', '.ogg', '.flac']:
            return 'audio_asset'
        else:
            return 'other'
    
    def _assess_competitive_value(self, file_path: str) -> float:
        """Assess competitive value of file"""
        category = self._categorize_file_enhanced(file_path)
        
        base_values = {
            'blend_native': 10.0,
            'blender_script': 8.0,
            '3d_model': 5.0,
            'texture_image': 4.0,
            'material_data': 3.5,
            'video_tutorial': 3.0,
            'data_file': 2.5,
            'python_script': 2.0,
            'audio_asset': 1.5,
            'other': 1.0
        }
        
        return base_values.get(category, 1.0)
    
    def _update_enhanced_stats(self):
        """Update enhanced statistics"""
        self.file_stats['total_files'] = len(self.selected_files)
        self.file_stats['total_size_mb'] = sum(f['size'] for f in self.selected_files) / (1024 * 1024)
        
        file_types = {}
        competitive_files = 0
        blender_4_5_files = 0
        total_business_value = 0.0
        
        for file_info in self.selected_files:
            category = file_info['category']
            competitive_value = file_info['competitive_value']
            
            file_types[category] = file_types.get(category, 0) + 1
            total_business_value += competitive_value
            
            if category in ['blend_native', 'blender_script'] or competitive_value >= 5.0:
                competitive_files += 1
            
            if self._is_blender_4_5_file(file_info['path']):
                blender_4_5_files += 1
        
        self.file_stats.update({
            'file_types': file_types,
            'competitive_files': competitive_files,
            'blender_4_5_files': blender_4_5_files,
            'business_value': total_business_value
        })
    
    def _is_blender_4_5_file(self, file_path: str) -> bool:
        """Check if file appears to be Blender 4.5 related"""
        filename = os.path.basename(file_path).lower()
        return any(term in filename for term in ['4.5', '4_5', 'latest', 'current', 'new'])
    
    def clear_files(self):
        self.selected_files.clear()
        self._update_enhanced_stats()
        debug.log("INFO", "Cleared all context files", "FILES")
    
    def get_files_for_context(self) -> List[str]:
        return [f['path'] for f in self.selected_files]
    
    def get_competitive_summary(self) -> Dict[str, Any]:
        """Get competitive analysis summary"""
        return {
            'total_files': self.file_stats['total_files'],
            'competitive_files': self.file_stats['competitive_files'],
            'blender_4_5_files': self.file_stats['blender_4_5_files'],
            'total_business_value': self.file_stats['business_value'],
            'competitive_percentage': (
                (self.file_stats['competitive_files'] / max(1, self.file_stats['total_files'])) * 100
                if self.file_stats['total_files'] > 0 else 0
            )
        }

# Global file manager
file_manager = EnhancedFileManager()

# =============================================================================
# UI PANELS - UPDATED FOR VISION INTEGRATION
# =============================================================================

class LLAMMY_PT_MainPanel(bpy.types.Panel):
    bl_label = "Llammy Vision-Integrated AI"
    bl_idname = "LLAMMY_PT_main_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    
    def draw(self, context):
        layout = self.layout
        
        # Header with system status
        header_box = layout.box()
        header_box.label(text="LLAMMY VISION-INTEGRATED AI v5.0", icon='CONSOLE')
        
        # Get comprehensive status
        status = module_manager.get_comprehensive_status()
        init_status = status.get('initialization_status', {})
        
        # Status indicators
        status_row = header_box.row()
        if init_status.get('core_initialized'):
            status_row.label(text="System: READY", icon='CHECKMARK')
        else:
            status_row.label(text="System: ERROR", icon='ERROR')
        
        # Module status row
        modules_row = header_box.row()
        components = ['Core', 'AI', 'RAG', 'Harvester']
        component_keys = ['core_initialized', 'ai_engine_connected', 'rag_integration_available', 'harvester_available']
        
        for comp, key in zip(components, component_keys):
            icon = 'CHECKMARK' if init_status.get(key, False) else 'X'
            modules_row.label(text=comp, icon=icon)
        
        # Vision integration status
        if init_status.get('vision_integration_active'):
            vision_row = header_box.row()
            vision_row.label(text="Vision: ACTIVE", icon='CAMERA_DATA')
        elif init_status.get('services_connected'):
            vision_row = header_box.row()
            vision_row.label(text="Vision: Basic Mode", icon='RESTRICT_VIEW_ON')
        
        # Performance metrics if available
        if init_status.get('core_initialized'):
            core_status = status.get('core_status', {}).get('core', {})
            stats = core_status.get('stats', {})
            
            if stats.get('total_requests', 0) > 0:
                success_rate = (stats.get('successful_requests', 0) / stats['total_requests']) * 100
                header_box.label(text=f"Success Rate: {success_rate:.1f}% ({stats['total_requests']} requests)")
                
                if stats.get('vision_enhanced_requests', 0) > 0:
                    vision_pct = (stats['vision_enhanced_requests'] / stats['total_requests']) * 100
                    header_box.label(text=f"Vision Enhanced: {vision_pct:.1f}%")

class LLAMMY_PT_ModelsPanel(bpy.types.Panel):
    bl_label = "AI Models Configuration"
    bl_idname = "LLAMMY_PT_models_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        scene = context.scene
        
        models_box = layout.box()
        models_box.label(text="Dual AI Configuration:", icon='SETTINGS')
        
        # Creative model selection
        creative_row = models_box.row()
        creative_row.label(text="Creative (Vision-Enhanced):", icon='BRUSH_DATA')
        if hasattr(scene, "llammy_creative_model"):
            models_box.prop(scene, "llammy_creative_model", text="")
        
        # Technical model selection
        technical_row = models_box.row()
        technical_row.label(text="Technical (Context-Aware):", icon='SETTINGS')
        if hasattr(scene, "llammy_technical_model"):
            models_box.prop(scene, "llammy_technical_model", text="")
        
        # Refresh button
        refresh_row = models_box.row()
        refresh_row.scale_y = 1.2
        refresh_row.operator("llammy.refresh_models", text="Refresh Models", icon='FILE_REFRESH')

class LLAMMY_PT_RequestPanel(bpy.types.Panel):
    bl_label = "AI Processing"
    bl_idname = "LLAMMY_PT_request_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        scene = context.scene
        
        request_box = layout.box()
        request_box.label(text="AI Request Input:", icon='CONSOLE')
        
        # Request input
        if hasattr(scene, "llammy_request_input"):
            request_box.prop(scene, "llammy_request_input", text="")
        
        # Context information
        file_stats = file_manager.get_competitive_summary()
        if file_stats['total_files'] > 0:
            context_info = request_box.row()
            context_text = f"Context: {file_stats['total_files']} files"
            if file_stats['competitive_files'] > 0:
                context_text += f" ({file_stats['competitive_files']} competitive)"
            context_info.label(text=context_text, icon='LIBRARY_DATA_DIRECT')
        
        # Vision status
        status = module_manager.get_comprehensive_status()
        if status.get('initialization_status', {}).get('vision_integration_active'):
            vision_row = request_box.row()
            vision_row.label(text="Scene Analysis: ACTIVE", icon='CAMERA_DATA')
        
        # Execute button
        execute_row = request_box.row()
        execute_row.scale_y = 2.0
        execute_row.operator("llammy.execute_request", text="EXECUTE VISION AI", icon='PLAY')

class LLAMMY_PT_ContentPanel(bpy.types.Panel):
    bl_label = "Content Context"
    bl_idname = "LLAMMY_PT_content_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        content_box = layout.box()
        
        file_stats = file_manager.get_competitive_summary()
        
        if file_stats['total_files'] > 0:
            content_box.label(text="Content Statistics:", icon='LIBRARY_DATA_DIRECT')
            
            stats_grid = content_box.grid_flow(row_major=True, columns=2, align=True)
            stats_grid.label(text="Total Files:")
            stats_grid.label(text=f"{file_stats['total_files']}")
            
            stats_grid.label(text="Competitive:")
            stats_grid.label(text=f"{file_stats['competitive_files']} ({file_stats['competitive_percentage']:.1f}%)")
            
            if file_stats['blender_4_5_files'] > 0:
                stats_grid.label(text="Blender 4.5:")
                stats_grid.label(text=f"{file_stats['blender_4_5_files']}")
            
            stats_grid.label(text="Business Value:")
            stats_grid.label(text=f"${file_stats['total_business_value']:.1f}")
        else:
            content_box.label(text="No Content Loaded", icon='LIBRARY_DATA_DIRECT')
        
        content_box.separator()
        content_box.label(text="Load Content:", icon='FILEBROWSER')
        
        file_row = content_box.row()
        file_col = file_row.column()
        file_col.operator("llammy.select_files", text="Select Files", icon='FILEBROWSER')
        file_col.operator("llammy.select_folder", text="Select Folder", icon='FILE_FOLDER')
        
        if file_stats['total_files'] > 0:
            clear_row = content_box.row()
            clear_row.operator("llammy.clear_files", text="Clear Content", icon='TRASH')

class LLAMMY_PT_SystemPanel(bpy.types.Panel):
    bl_label = "System Status"
    bl_idname = "LLAMMY_PT_system_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    def draw(self, context):
        layout = self.layout
        status_box = layout.box()
        
        status = module_manager.get_comprehensive_status()
        init_status = status.get('initialization_status', {})
        
        status_box.label(text="Module Status:", icon='SYSTEM')
        
        # Service status
        services_row = status_box.row()
        services_icon = 'CHECKMARK' if init_status.get('services_connected') else 'X'
        services_row.label(text="Centralized Services:", icon=services_icon)
        
        # Vision status
        vision_row = status_box.row()
        vision_icon = 'CHECKMARK' if init_status.get('vision_integration_active') else 'RESTRICT_VIEW_ON'
        vision_status = "ACTIVE" if init_status.get('vision_integration_active') else "Basic Mode"
        vision_row.label(text=f"Vision Integration: {vision_status}", icon=vision_icon)
        
        # Core modules
        for label, key in [("Core:", 'core_initialized'), ("AI Engine:", 'ai_engine_connected'),
                          ("RAG:", 'rag_integration_available'), ("Harvester:", 'harvester_available')]:
            row = status_box.row()
            icon = 'CHECKMARK' if init_status.get(key) else 'X'
            row.label(text=label, icon=icon)

class LLAMMY_PT_DebugPanel(bpy.types.Panel):
    bl_label = "Debug Information"
    bl_idname = "LLAMMY_PT_debug_panel"
    bl_space_type = 'VIEW_3D'
    bl_region_type = 'UI'
    bl_category = 'Llammy'
    bl_parent_id = "LLAMMY_PT_main_panel"
    
    @classmethod
    def poll(cls, context):
        return DEBUG_ENABLED
    
    def draw(self, context):
        layout = self.layout
        debug_box = layout.box()
        debug_box.label(text="Recent Debug Logs:", icon='INFO')
        
        recent_logs = debug.get_recent_logs(8)
        for log_entry in recent_logs:
            display_entry = log_entry[:80] + "..." if len(log_entry) > 80 else log_entry
            debug_box.label(text=display_entry)

# =============================================================================
# OPERATORS - UPDATED FOR SERVICE INTEGRATION
# =============================================================================

class LLAMMY_OT_ExecuteRequest(bpy.types.Operator):
    bl_idname = "llammy.execute_request"
    bl_label = "Execute AI Request"
    bl_description = "Process request through vision-integrated AI system"
    
    def execute(self, context):
        scene = context.scene
        user_request = getattr(scene, 'llammy_request_input', '').strip()
        
        if not user_request:
            self.report({'ERROR'}, "Please enter a request")
            return {'CANCELLED'}
        
        try:
            creative_model = getattr(scene, 'llammy_creative_model', 'unknown')
            technical_model = getattr(scene, 'llammy_technical_model', 'unknown')
            
            self.report({'INFO'}, f"Processing with vision integration...")
            
            # Report context information
            file_stats = file_manager.get_competitive_summary()
            if file_stats['total_files'] > 0:
                self.report({'INFO'}, f"Context: {file_stats['total_files']} files")
            
            # Prepare request parameters
            request_params = {
                'creative_model': creative_model,
                'technical_model': technical_model,
                'models_used': {
                    'creative': creative_model,
                    'technical': technical_model
                }
            }
            
            # Add file context if available
            file_paths = file_manager.get_files_for_context()
            if file_paths:
                request_params['file_context'] = file_paths
            
            # Process request through module manager
            result = module_manager.process_request(user_request, **request_params)
            
            if hasattr(scene, 'llammy_last_result'):
                scene.llammy_last_result = str(result)
            
            if result.get('success'):
                processing_time = result.get('processing_time', 0)
                self.report({'INFO'}, f"Processing complete in {processing_time:.2f}s")
                
                if result.get('vision_enhanced'):
                    self.report({'INFO'}, "Vision enhancement applied")
                
                generated_code = result.get('generated_code', '')
                
                if generated_code:
                    try:
                        self.report({'INFO'}, "Executing generated code...")
                        exec(generated_code)
                        
                        method = result.get('method', 'unknown')
                        self.report({'INFO'}, f"SUCCESS! Generated via {method}")
                        
                        if hasattr(scene, 'llammy_request_input'):
                            scene.llammy_request_input = ""
                        
                    except Exception as exec_error:
                        self.report({'WARNING'}, f"Code execution error: {str(exec_error)}")
                        debug.log("ERROR", f"Code execution failed: {exec_error}", "EXECUTE")
                else:
                    self.report({'WARNING'}, "AI generated empty code")
            else:
                error_msg = result.get('error', 'Unknown error')
                self.report({'ERROR'}, f"AI request failed: {error_msg}")
                
        except Exception as e:
            self.report({'ERROR'}, f"Request processing failed: {str(e)}")
            debug.log("ERROR", f"Execute operator failed: {e}", "EXECUTE")
        
        return {'FINISHED'}

class LLAMMY_OT_RefreshModels(bpy.types.Operator):
    bl_idname = "llammy.refresh_models"
    bl_label = "Refresh Models"
    bl_description = "Refresh available Ollama models via centralized service"
    
    def execute(self, context):
        try:
            self.report({'INFO'}, "Refreshing model list via centralized service...")
            
            # Force refresh through service
            if module_manager.ollama_service:
                models = module_manager.ollama_service.get_models_for_blender(force_refresh=True)
                self.report({'INFO'}, f"Found {len(models)} models via service")
            
            # Re-register properties to refresh enum items
            bpy.types.Scene.llammy_creative_model = bpy.props.EnumProperty(
                name="Creative Model",
                description="Model for creative analysis with vision enhancement",
                items=get_ollama_models_for_properties
            )
            bpy.types.Scene.llammy_technical_model = bpy.props.EnumProperty(
                name="Technical Model",
                description="Model for technical code generation with context awareness",
                items=get_ollama_models_for_properties
            )
            
            self.report({'INFO'}, "Model list refreshed successfully")
            debug.log("SUCCESS", "Models refreshed via service", "MODELS")
            
        except Exception as e:
            self.report({'WARNING'}, f"Refresh failed: {str(e)}")
            debug.log("ERROR", f"Model refresh failed: {e}", "MODELS")
        
        return {'FINISHED'}

class LLAMMY_OT_SelectFiles(bpy.types.Operator):
    bl_idname = "llammy.select_files"
    bl_label = "Select Files"
    bl_description = "Select files for AI context enhancement"
    bl_options = {'REGISTER'}
    
    files: bpy.props.CollectionProperty(
        name="File Path",
        type=bpy.types.OperatorFileListElement,
    )
    
    directory: bpy.props.StringProperty(subtype='DIR_PATH')
    
    filter_glob: bpy.props.StringProperty(
        default="*.blend;*.py;*.obj;*.fbx;*.jpg;*.jpeg;*.png;*.exr;*.json;*.csv;*.txt;*.mp4;*.wav;*.md;*.yaml",
        options={'HIDDEN'}
    )
    
    def execute(self, context):
        file_paths = []
        for file_elem in self.files:
            file_path = os.path.join(self.directory, file_elem.name)
            if os.path.exists(file_path):
                file_paths.append(file_path)
        
        if file_paths:
            file_manager.add_files(file_paths)
            file_stats = file_manager.get_competitive_summary()
            
            self.report({'INFO'}, f"Added {len(file_paths)} files")
            if file_stats['competitive_files'] > 0:
                self.report({'INFO'}, f"Competitive files: {file_stats['competitive_files']}")
        else:
            self.report({'WARNING'}, "No files selected")
        
        return {'FINISHED'}
    
    def invoke(self, context, event):
        context.window_manager.fileselect_add(self)
        return {'RUNNING_MODAL'}

class LLAMMY_OT_SelectFolder(bpy.types.Operator):
    bl_idname = "llammy.select_folder"
    bl_label = "Select Folder"
    bl_description = "Select folder for batch file processing"
    bl_options = {'REGISTER'}
    
    directory: bpy.props.StringProperty(
        name="Folder Path",
        subtype='DIR_PATH',
    )
    
    def execute(self, context):
        if not self.directory or not os.path.exists(self.directory):
            self.report({'ERROR'}, "Invalid folder selected")
            return {'CANCELLED'}
        
        file_paths = []
        folder_path = Path(self.directory)
        
        supported_exts = {
            '.blend', '.py', '.obj', '.fbx', '.dae', '.ply', '.stl', '.gltf', '.glb',
            '.jpg', '.jpeg', '.png', '.tga', '.exr', '.hdr', '.bmp', '.tiff',
            '.json', '.xml', '.mtl', '.csv', '.txt', '.md', '.yaml', '.ini',
            '.mp4', '.avi', '.mov', '.mkv', '.webm', '.wav', '.mp3', '.ogg', '.flac'
        }
        
        try:
            for file_path in folder_path.rglob('*'):
                if (file_path.is_file() and
                    file_path.suffix.lower() in supported_exts):
                    file_paths.append(str(file_path))
                    
                    if len(file_paths) >= 200:
                        break
            
            if file_paths:
                file_manager.add_files(file_paths)
                file_stats = file_manager.get_competitive_summary()
                
                self.report({'INFO'}, f"Loaded {len(file_paths)} files from folder")
                if file_stats['competitive_files'] > 0:
                    self.report({'INFO'}, f"Found {file_stats['competitive_files']} competitive files")
                
                if len(file_paths) >= 200:
                    self.report({'WARNING'}, "Limited to 200 files - folder contains more")
            else:
                self.report({'WARNING'}, "No supported files found in folder")
                
        except Exception as e:
            self.report({'ERROR'}, f"Folder processing failed: {str(e)}")
        
        return {'FINISHED'}
    
    def invoke(self, context, event):
        context.window_manager.fileselect_add(self)
        return {'RUNNING_MODAL'}

class LLAMMY_OT_ClearFiles(bpy.types.Operator):
    bl_idname = "llammy.clear_files"
    bl_label = "Clear Files"
    bl_description = "Clear all loaded context files"
    
    def execute(self, context):
        files_count = file_manager.file_stats['total_files']
        file_manager.clear_files()
        self.report({'INFO'}, f"Cleared {files_count} files from context")
        debug.log("INFO", f"Cleared {files_count} context files", "FILES")
        return {'FINISHED'}

# =============================================================================
# PROPERTIES REGISTRATION - CONFLICT-FREE
# =============================================================================

def register_properties():
    """Register all Blender properties with conflict-free model enumeration"""
    try:
        bpy.types.Scene.llammy_request_input = bpy.props.StringProperty(
            name="AI Request",
            description="Your request for the vision-integrated AI system",
            default="",
            maxlen=2000
        )
        
        bpy.types.Scene.llammy_creative_model = bpy.props.EnumProperty(
            name="Creative Model",
            description="Model for creative analysis with vision enhancement",
            items=get_ollama_models_for_properties
        )
        
        bpy.types.Scene.llammy_technical_model = bpy.props.EnumProperty(
            name="Technical Model",
            description="Model for technical code generation with context awareness",
            items=get_ollama_models_for_properties
        )
        
        bpy.types.Scene.llammy_last_result = bpy.props.StringProperty(
            name="Last Result",
            description="Result from last AI request",
            default=""
        )
        
        debug.log("SUCCESS", "Properties registered successfully", "PROPS")
        
    except Exception as e:
        debug.log("ERROR", f"Property registration failed: {e}", "PROPS")

def unregister_properties():
    """Unregister all Blender properties"""
    properties = [
        'llammy_request_input',
        'llammy_creative_model',
        'llammy_technical_model',
        'llammy_last_result'
    ]
    
    for prop in properties:
        try:
            if hasattr(bpy.types.Scene, prop):
                delattr(bpy.types.Scene, prop)
                debug.log("SUCCESS", f"Unregistered property: {prop}", "PROPS")
        except Exception as e:
            debug.log("ERROR", f"Failed to unregister {prop}: {e}", "PROPS")

# =============================================================================
# ADDON REGISTRATION
# =============================================================================

classes = [
    LLAMMY_PT_MainPanel,
    LLAMMY_PT_ModelsPanel,
    LLAMMY_PT_RequestPanel,
    LLAMMY_PT_ContentPanel,
    LLAMMY_PT_SystemPanel,
    LLAMMY_PT_DebugPanel,
    LLAMMY_OT_ExecuteRequest,
    LLAMMY_OT_RefreshModels,
    LLAMMY_OT_SelectFiles,
    LLAMMY_OT_SelectFolder,
    LLAMMY_OT_ClearFiles,
]

def register():
    """Register the vision-integrated Llammy addon"""
    debug.log("INFO", "Starting vision-integrated addon registration", "REGISTER")
    
    # Register UI classes
    for cls in classes:
        try:
            bpy.utils.register_class(cls)
            debug.log("SUCCESS", f"Registered UI class: {cls.__name__}", "REGISTER")
        except Exception as e:
            debug.log("ERROR", f"Failed to register {cls.__name__}: {e}", "REGISTER")
    
    # Register properties
    register_properties()
    
    # Initialize all modules with services
    success = module_manager.initialize_all_modules()
    
    # Print registration summary
    print("=" * 80)
    print("LLAMMY VISION-INTEGRATED AI v5.0 REGISTERED")
    print("=" * 80)
    print("RE-ENGINEERED ARCHITECTURE:")
    print("  - Centralized Ollama service (no conflicts)")
    print("  - Vision Intelligence Service integration")
    print("  - Enhanced context with scene awareness")
    print("  - Service-integrated AI processing")
    print("  - Conflict-free model enumeration")
    print("  - Vision LoRA framework integration")
    print("")
    print("CORE FEATURES:")
    print("  - Dual AI with vision enhancement")
    print("  - Scene-aware code generation")
    print("  - Context-driven model selection")
    print("  - Intelligent harvesting with psutil fallback")
    print("  - RAG with competitive analysis")
    print("  - Triangle104 model prioritization")
    
    if success:
        status = module_manager.get_comprehensive_status()
        init_status = status.get('initialization_status', {})
        
        print("")
        print("SYSTEM STATUS:")
        if init_status.get('vision_integration_active'):
            print("  - Vision Integration: ACTIVE")
        else:
            print("  - Vision Integration: Basic Mode")
        
        if init_status.get('services_connected'):
            print("  - Centralized Services: CONNECTED")
        
        operational_modules = sum(1 for v in [
            init_status.get('core_initialized'),
            init_status.get('ai_engine_connected'),
            init_status.get('rag_integration_available'),
            init_status.get('harvester_available')
        ] if v)
        
        print(f"  - Operational Modules: {operational_modules}/4")
        print("  - All Core Systems: OPERATIONAL")
    else:
        print("")
        print("SYSTEM STATUS:")
        print("  - Some Systems: LIMITED FUNCTIONALITY")
        print("   Check System Status panel for details")
    
    if DEBUG_ENABLED:
        print("  - Debug Mode: ENABLED")
    
    print("=" * 80)
    print("Ready for vision-enhanced Blender AI automation!")

def unregister():
    """Unregister the vision-integrated addon"""
    debug.log("INFO", "Starting addon unregistration", "UNREGISTER")
    
    # Unregister properties
    unregister_properties()
    
    # Unregister UI classes
    for cls in reversed(classes):
        try:
            bpy.utils.unregister_class(cls)
            debug.log("SUCCESS", f"Unregistered: {cls.__name__}", "UNREGISTER")
        except Exception as e:
            debug.log("ERROR", f"Failed to unregister {cls.__name__}: {e}", "UNREGISTER")
    
    # Shutdown harvester if available
    if module_manager.harvester and hasattr(module_manager.harvester, 'shutdown'):
        try:
            module_manager.harvester.shutdown()
            debug.log("SUCCESS", "Harvester shutdown complete", "UNREGISTER")
        except Exception as e:
            debug.log("ERROR", f"Harvester shutdown failed: {e}", "UNREGISTER")
    
    print("Llammy Vision-Integrated AI unregistered successfully")

# =============================================================================
# DIRECT EXECUTION SUPPORT
# =============================================================================

if __name__ == "__main__":
    register()

# =============================================================================
# MODULE COMPLETION NOTICE
# =============================================================================

print("LLAMMY VISION-INTEGRATED AI v5.0 - RE-ENGINEERED ARCHITECTURE LOADED!")
print("Complete conflict-free integration:")
print("  - No duplicate Ollama calls")
print("  - Centralized service architecture")
print("  - Vision LoRA framework integration")
print("  - Enhanced scene-aware AI processing")
print("  - Proper dependency management")
print("  - Service-connected module ecosystem")
print("Ready for vision-enhanced 3D modeling automation!")
